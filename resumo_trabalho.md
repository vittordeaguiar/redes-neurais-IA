# RESUMO DO TRABALHO - Rede Neural em Prolog

## Enunciado no Google Classroom:
"Entrega de um v√≠deo descritivo e anal√≠tico (com aproximadamente 15 minutos de dura√ß√£o) com explica√ß√µes e reflex√µes dos grupos de trabalho. Aten√ß√£o: todos os integrantes dos grupos dever√£o participar das apresenta√ß√µes em v√≠deo.
Experimenta√ß√£o de Redes MLP (no Weka) para reconhecer letras (de A a Z) do nosso alfabeto.

Crit√©rios de Avalia√ß√£o:
- Elabora√ß√£o e representa√ß√£o do dataset de todas as letras (A a Z) (3,0)
- Experimenta√ß√µes e reflex√µes acerca do processo de modelagem, aprendizagem e valida√ß√£o das redes neurais. (7,0)"

## Informa√ß√µes Gerais
- **Disciplina:** Intelig√™ncia Artificial (Faculdade)
- **Objetivo:** Criar rede neural em Prolog para reconhecer as 26 letras do alfabeto (A-Z)
- **Representa√ß√£o:** Matriz 5x5 (25 pixels), valores 1 e -1
- **Entreg√°veis:** Dataset + C√≥digo Prolog + V√≠deo explicativo

---

## ‚úÖ O QUE J√Å FOI FEITO

### 1. Dataset Criado
- **26 letras** do alfabeto desenhadas em matriz 5x5
- **6 varia√ß√µes por letra** (1 limpa + 5 com ru√≠do de 1-3 pixels)
- **156 registros totais**
- Formatos: ARFF (WEKA) e fatos Prolog

### 2. Rede Neural Implementada
- **Arquitetura:** Perceptron multicamada (one-vs-all)
- **Entradas:** 25 neur√¥nios (pixels)
- **Sa√≠das:** 26 classificadores bin√°rios (um por letra)
- **Fun√ß√£o de ativa√ß√£o:** Degrau (step function)
- **F√≥rmula:** `novo_peso = peso_atual + (taxa √ó erro √ó entrada)`

### 3. Funcionalidades do C√≥digo
- Menu interativo completo
- Inicializa√ß√£o de pesos aleat√≥rios
- Treinamento por √©pocas configur√°vel
- Avalia√ß√£o com acur√°cia
- Avalia√ß√£o detalhada (mostra erros)
- Teste visual de letras espec√≠ficas
- Compara√ß√£o autom√°tica de taxas de aprendizado
- Classifica√ß√£o de entrada manual

### 4. Documenta√ß√£o Criada
- Guia de instala√ß√£o do SWI-Prolog
- Roteiro detalhado para grava√ß√£o do v√≠deo
- Documenta√ß√£o visual das 26 letras

---

## üìÅ ARQUIVOS GERADOS

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `rede_neural.pl` | **C√≥digo principal** - rede neural completa com dataset embutido |
| `letras_dataset.arff` | Dataset formato WEKA (156 exemplos com ru√≠do) |
| `letras_dataset.pl` | Dataset formato Prolog separado |
| `letras_base.arff` | Dataset s√≥ com letras limpas (26 exemplos) |
| `letras_5x5.md` | Documenta√ß√£o visual de todas as 26 letras |
| `gerar_dataset.py` | Script Python para regenerar/ajustar dataset |
| `GUIA_USO.md` | Guia completo de instala√ß√£o e uso |
| `ROTEIRO_VIDEO.md` | Roteiro detalhado para grava√ß√£o do v√≠deo |

---

## üîß COMO USAR

### Instala√ß√£o
1. Baixar SWI-Prolog: https://www.swi-prolog.org/download/stable
2. Instalar normalmente no Windows

### Execu√ß√£o
```prolog
% 1. Abrir SWI-Prolog

% 2. Carregar arquivo (usar / no caminho)
?- ['C:/SuaPasta/rede_neural.pl'].

% 3. Op√ß√£o r√°pida - executa tudo
?- executar_tudo.

% 4. Ou usar menu interativo
?- menu.
```

### Comandos Principais
```prolog
?- inicializar_rede.      % Cria pesos aleat√≥rios
?- treinar(100).          % Treina 100 √©pocas
?- avaliar.               % Mostra acur√°cia
?- avaliar_detalhado.     % Mostra erros espec√≠ficos
?- testar_visual(a).      % Testa letra espec√≠fica
?- comparar_taxas.        % Compara learning rates
```

---

## üé• ROTEIRO DO V√çDEO (10-15 min)

### Estrutura
1. **Introdu√ß√£o (2 min)** - Explicar o problema e objetivo
2. **Dataset (2 min)** - Mostrar estrutura das letras 5x5
3. **Arquitetura (2 min)** - Explicar Perceptron e f√≥rmulas
4. **Demonstra√ß√£o (5 min)** - Executar no SWI-Prolog ao vivo
5. **Learning Rate (3 min)** - Mostrar `comparar_taxas` e analisar
6. **Conclus√£o (1 min)** - Resumir resultados

### Sequ√™ncia de Comandos para Demonstra√ß√£o
```
1. Carregar arquivo
2. Op√ß√£o 1: Inicializar rede
3. Op√ß√£o 3: Avaliar (antes de treinar ~5%)
4. Op√ß√£o 2: Treinar 10 √©pocas
5. Op√ß√£o 3: Avaliar (~50%)
6. Op√ß√£o 2: Treinar mais 90 √©pocas
7. Op√ß√£o 4: Avalia√ß√£o detalhada (~95-99%)
8. Op√ß√£o 5: Testar letras a, m, z
9. Op√ß√£o 9: Comparar taxas de aprendizado ‚Üê IMPORTANTE!
```

### Software para Gravar
- **Windows:** Win+G (Xbox Game Bar) ou OBS Studio
- **Dica:** Aumentar fonte do Prolog para ficar leg√≠vel

---

## üìä RESULTADOS ESPERADOS

### Acur√°cia por Fase
| Momento | Acur√°cia Esperada |
|---------|-------------------|
| Antes de treinar | ~5-10% |
| Ap√≥s 10 √©pocas | ~40-60% |
| Ap√≥s 100 √©pocas | ~95-99% |

### Compara√ß√£o de Learning Rates (50 √©pocas)
| Taxa | Resultado |
|------|-----------|
| 0.01 | ~45% (muito lenta) |
| 0.05 | ~75% |
| 0.1 | ~90% |
| 0.2 | ~95% (ideal) |
| 0.3 | ~85% |
| 0.5 | ~70% (inst√°vel) |

### Erros Comuns
- B ‚Üî D (formato similar)
- O ‚Üî Q (diferem s√≥ na "cauda")
- Letras com poucos pixels caracter√≠sticos

---

## ‚è≥ PR√ìXIMOS PASSOS

1. ‚úÖ ~~Desenhar as 26 letras base~~
2. ‚úÖ ~~Gerar varia√ß√µes com ru√≠do~~
3. ‚úÖ ~~Montar dataset (ARFF e Prolog)~~
4. ‚úÖ ~~Implementar rede neural~~
5. ‚è≥ **Testar no SWI-Prolog local**
6. ‚è≥ **Ajustar par√¢metros se necess√°rio**
7. ‚è≥ **Gravar o v√≠deo**
8. ‚è≥ **Entregar trabalho**

---

## üí° CONCEITOS PARA EXPLICAR NO V√çDEO

### Taxa de Aprendizado (Learning Rate)
- Controla o "tamanho do passo" no ajuste dos pesos
- Muito alta ‚Üí oscila, n√£o converge
- Muito baixa ‚Üí aprende devagar demais
- Ideal para este problema: 0.1 a 0.2

### √âpoca
- Uma passagem completa por todos os exemplos do dataset
- 100 √©pocas = ver todos os 156 exemplos 100 vezes

### One-vs-All
- Estrat√©gia para classifica√ß√£o multiclasse
- 26 classificadores bin√°rios independentes
- Cada um aprende "√© letra X?" vs "n√£o √© letra X?"

### Ru√≠do no Dataset
- Simula imperfei√ß√µes do mundo real
- Ajuda a rede generalizar melhor
- Evita overfitting (decorar os exemplos)

---

## üîó LINKS √öTEIS

- **SWI-Prolog Download:** https://www.swi-prolog.org/download/stable
- **Documenta√ß√£o SWI-Prolog:** https://www.swi-prolog.org/pldoc/
- **OBS Studio (gravador):** https://obsproject.com/

---

*√öltima atualiza√ß√£o: Novembro 2024*
